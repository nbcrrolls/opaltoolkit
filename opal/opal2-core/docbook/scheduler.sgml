<chapter id="scheduler" xreflabel="Scheduler Support Using Job Managers">
<title>Scheduler Support Using Job Managers</title>

<para>In this chapter, we describe how to configure Opal so that it can
submit jobs to an external scheduler, using the various job managers
provided. We also provide a brief introduction on how to write your own job
manager, if need be.</para>

<para>The prerequisite to this section is that you have a scheduler such as
Condor or SGE installed on your compute cluster. Access to schedulers is
provided in a number of ways - in particular, via the DRMAA API and the
Globus Toolkit. Starting with release 2.1, we support Condor natively
without using DRMAA or Globus, and also the Community Scheduler Framework
(CSF4) for scheduling jobs across multiple clusters. In release 2.3, we
introduced direct support for TORQUE/PBS without having to use DRMAA or
Globus. Please consult the appropriate documentation for the installation
of these scheduler tools.</para>

<section id="drmaa-scheduler" xreflabel="DRMAA Setup">
<title>Using DRMAA</title>

<orderedlist>
<listitem><para>Ensure that your scheduler supports job submission via the DRMAA API.
Also ensure that <emphasis role="italics">libdrmaa.so</emphasis> is in your library path (by setting
 the LD_LIBRARY_PATH environment variable). We have only tested job
 submissions to SGE 6.x.</para></listitem>

<listitem><para>Set the following properties inside the opal.properties
file: <filename>opal.jobmanager</filename> to
<filename>edu.sdsc.nbcr.opal.manager.DRMAAJobManager</filename>, and
<filename>drmaa.pe</filename> to the name of the parallel environment (PE)
to be used to submit parallel jobs. You can ignore drmaa.pe if you do not
plan on supporting parallel jobs. </para></listitem>

<listitem><para>Reinstall Opal by running the following command:
<screen>
    ant install
</screen>
</para>

</listitem>

<listitem><para>Restart Tomcat for the changes to take effect.</para></listitem>

<listitem><para>Note that we do not stage input and output files from the
Opal server to the submit host if DRMAA is being used; hence, the machines
should be on a shared file system for the DRMAA job manager to work
correctly.</para> 

<para>Also note that DRMAA support will not work correctly if you have
another version of the drmaa.jar inside Tomcat's common/lib or server/lib
directories, since that will be loaded first by Tomcat's class
loader.</para></listitem>
</orderedlist>
</section>

<section id="globus-scheduler" xreflabel="Globus Setup">
<title>Using Globus</title>

<orderedlist>
<listitem><para>Install Globus (especially the Globus gatekeeper) on the head node of
the cluster. We use Globus GRAM to communicate programmatically with
Condor/SGE. To enable this, make sure that you have also installed the
Condor/SGE job-manager. Follow the documentation for this on the Globus web
site. You may install any version of Globus as long as you can submit jobs
to Condor/SGE via GRAM (and the Java CoG Kit). We use Globus version 3.2.0
on our cluster.</para>

<para>Ensure that you can submit jobs to Condor/SGE via Globus, especially
using the certificate/key-pair of the Tomcat server (described in  <xref linkend="security">
). You can do this by following these
steps:</para>

<orderedlist>
  <listitem><para>Copy app_service.cert.pem (the certificate file) to
  app_service.all.pem.</para></listitem>

  <listitem><para>Edit app_service.all.pem, and strip out everything excluding the
  region between the lines <filename>-----BEGIN CERTIFICATE-----</filename> and
  <filename>-----END CERTIFICATE-----</filename>. Leave those two lines in.</para></listitem>

  <listitem><para>Append the app_service.privkey (the unencrypted private key) to the
  contents of app_service.all.pem.</para></listitem>

  <listitem><para>Set the X509_USER_PROXY environment variable to the location of
  app_service.all.pem.</para></listitem>

  <listitem><para>Submit a test job using the above proxy to the Condor job-manager as
  follows: <filename>globus-job-run "hostname:2119/jobmanager-condor"
  "/bin/ls"</filename>. If you are using SGE, use
  <filename>"hostname:2119/jobmanager-sge"</filename>.</para>

  <para>If this above job succeeds, then Globus/Condor(SGE) can be used for
  scheduling purposes. You will have to add an entry into the grid-mapfile
  of the Globus installation (usually inside the /etc/grid-security
  directory) to authorize the service to launch jobs as follows:
  <filename>"/C=US/O=grid-devel/OU=sdsc/CN=app_service"
  app_user</filename>. Replace the value within quotes with the DN for the
  app_service.cert.pem. You can get the DN by running:
  <filename>grid-cert-info -subject -file app_service.cert.pem</filename>.
  The app_user is the Unix user running the Tomcat server hosting Opal.</para>

  <para>You may also have to add the CA cert and signing policy of the above
  certificate into the list of trusted certificates for the Globus
  installation (usually inside the /etc/grid-security/certificates
  directory).</para>

  <para>You may also want to check if you can submit parallel jobs via
  Globus. You can do so by running something like this: <filename>globusrun -o -r
  hostname:2119/jobmanager-sge
  "&amp;(executable=&lt;my_mpi_exec&gt;)(count=n)(jobtype=mpi)"</filename>. Replace
  <filename>&lt;my_mpi_exec&gt;</filename> with some valid MPI executable, and replace
  count with a valid number of processes for your executable</para></listitem>
</orderedlist>
</listitem>

<listitem><para>Set the following properties inside the opal.properties
file: <filename>globus.gatekeeper</filename> to the URL for the Globus
gatekeeper, <filename>globus.service_cert</filename> to the location of the
server certificate, and <filename>globus.service_privkey</filename> to the
location of the server's unencrypted private key.</para></listitem>

<listitem><para>If you would like to submit Globus jobs to a local cluster,
then set the property <filename>opal.jobmanager</filename> to
<filename>edu.sdsc.nbcr.opal.manager.GlobusJobManager</filename>, and if it
is to a remote cluster, then set it to
<filename>edu.sdsc.nbcr.opal.manager.RemoteGlobusJobManager</filename>. If
you are using the remote Globus job manager, set the
<filename>globus.gridftp_base</filename> to the base URL of the location to
stage files to. Make sure that this is a valid directory, and that the
app_user can write files can create directories in this location. It is
important to notice that if you specify a path with
gsiftp://myhost.mydomain.com:2812/data/foo.dat, the location 'data/foo.dat'
will be relative to the remote home directory. To point to absolute paths,
you have to use double slashes (//), e.g.
gsiftp://myhost.mydomain.com:2812//data/foo.dat.</para></listitem>

<listitem><para>Reinstall Opal by running the following command:
<screen>
    ant install
</screen>
</para>
</listitem>

<listitem><para>Restart Tomcat for the changes to take effect.</para></listitem>
</orderedlist>

</section>

<section id="condor-scheduler" xreflabel="Condor Setup">
<title>Using Condor</title>

<orderedlist>
<listitem><para>Ensure that you have a working Condor pool by following the
Condor <ulink type="http"
url="http://www.cs.wisc.edu/condor/manual/v7.0.5/">manual</ulink>. We have
tested with version 7.0.5, but we expect that it will work with most recent
Condor versions. In particular, verify that you can submit and monitor jobs
from the command-line with commands such as
<filename>condor_submit</filename>, <filename>condor_status</filename>,
etc., and that these commands are in the Opal user's
PATH.</para></listitem>

<listitem><para>Set the following properties inside the opal.properties
file: <filename>opal.jobmanager</filename> to
<filename>edu.sdsc.nbcr.opal.manager.CondorJobManager</filename>, and
<filename>mpi.script</filename> to the script used by Condor to submit
parallel jobs. You can ignore mpi.script if you do not plan on supporting
parallel jobs. </para></listitem>

<listitem><para>Reinstall Opal by running the following command:
<screen>
    ant install
</screen>
</para>
</listitem>

<listitem><para>Restart Tomcat for the changes to take effect.</para></listitem>

<listitem><para>Note that we have not tested advanced Condor features such
as flocking and Condor-G.</para></listitem>
</orderedlist>

</section>

<section id="pbs-scheduler" xreflabel="TORQUE/PBS Setup">
<title>Using TORQUE/PBS</title>

<orderedlist>
<listitem><para>Ensure that you have a working TORQUE/PBS installation, by following the instructions on the TORQUE <ulink type="http" url="http://www.clusterresources.com/products/torque-resource-manager.php">website</ulink>. In particular, verify that you can submit and monitor jobs from the command-line with commands such as <filename>qsub</filename> and <filename>qstat</filename>. Note that we have only tested with version 2.3.0 of TORQUE.</para></listitem>

<listitem><para>Inside the opal.properties, set the
<filename>opal.jobmanager</filename> to
<filename>edu.sdsc.nbcr.opal.manager.PBSJobManager</filename>.</para></listitem>

<listitem><para>Reinstall Opal by running the following command:
<screen>
    ant install
</screen>
</para>
</listitem>

<listitem><para>Make sure that you all the PBS binaries such as
<filename>qsub</filename> and <filename>qstat</filename> in your
PATH.</para></listitem>

<listitem><para>Restart Tomcat for the changes to take effect.</para></listitem>

</orderedlist>

</section>

<section id="csf4-scheduler" xreflabel="CSF Setup">
<title>Using CSF4</title>

<para>
The <ulink url="http://sourceforge.net/projects/gcsf/"> 
Community Scheduler Framework (CSF)</ulink> is a set of Grid Services, 
implemented using the Globus Toolkit, which provides an environment 
for the development of metaschedulers that can dispatch jobs to 
resource managers such as LSF, SGE, PBS and Condor.
</para>

<para>
The Opal CSF plugin has been tested with CSF 4.0.5.1.
You can download CSF 4.0.5.1 as well as its documentation from
<ulink url="http://sourceforge.net/projects/gcsf/files/">
the CSF Source Forge download page</ulink>. 
</para>

<orderedlist>
<listitem><para>
The CSF plugin requires a valid proxy from the user who starts Tomcat.
Please see <link linkend='proxy-script'>Appendix on proxy scripts</link> 
for a sample script to generate proxy.
</para></listitem>
<listitem><para>In the opal.properties
file, set <filename>opal.jobmanager</filename> to
<filename>edu.sdsc.nbcr.opal.manager.CSFJobManager</filename>.
</para></listitem>
<listitem><para>
Create an Opal directory on all machines that CSF4 will be scheduling
jobs to. The Opal job manager requires these directories to have the 
same name on all resources.
In <filename>opal.properties</filename>, set 
<filename>csf4.workingDir</filename> to the relative path of this 
working directory with respect to the home directory of the CSF4 user.
</para></listitem>
<listitem><para>
If you have already sourced 
<filename>$GLOBUS_LOCATION/etc/globus-user-env.sh</filename> and
<filename>$GLOBUS_LOCATION/etc/globus-devel-env.sh</filename>,
then you need to unset the CLASSPATH.  This is because there are 
jar conflicts between the library files used by gt4 and the library
files used by Opal.  The command below unsets the CLASSPATH.
<screen>
export CLASSPATH=""
</screen>
</para></listitem>
<listitem><para>
Reinstall Opal by running the following command:
<screen>
ant install
</screen>
</para></listitem>
<listitem><para>
Before restarting Tomcat, ensure that the environment for Globus and CSF4 is set up correctly by execute the following command:
<screen>
source $GLOBUS_LOCATION/etc/globus-user-env.sh
source $GLOBUS_LOCATION/etc/globus-devel-env.sh
export CSF_CLASSPATH=$CLASSPATH
</screen>
</para></listitem>
<listitem><para>
Restart Tomcat for changes to take effect.
</para></listitem>
</orderedlist>

<para>
In your application configuration files, the <filename>binaryLocation</filename>
element needs to be of the form <filename>$APPNAME:$BINARY</filename>.
For example,
<screen>
&lt;binaryLocation&gt;PDB2PQR:pdb2pqr.py&lt;/binaryLocation&gt;
</screen>
</para>

<para>
This is because CSF4 can figure out the exact path of the application
on a particular resource using the <filename>$APPNAME</filename>.
</para>

</section>

<section id="jobmanager-howto" xreflabel="Writing Your Own Job Manager">
<title>Writing Your Own Job Manager</title>

<para>If the job managers provided by the Opal toolkit are not sufficient
for your purposes, you can write your own job managers to submit jobs to
your favorite scheduler. This section presents a brief tutorial on how to
do so.</para>

<para>To write your own Opal job manager, you must implement the
<filename>edu.sdsc.nbcr.opal.manager.OpalJobManager</filename> interface.
One job manager is instantiated per job instance - i.e. for every run of an
application, a new job manager is created. The OpalJobManager interface
that 6 methods that must be implemented:</para>

<orderedlist>

<listitem><para>The <filename>initialize</filename> method initializes the
job manager, by setting a list of properties (key/value pairs), application
configuration, and an optional handle. All job manager specific properties
should be put inside <filename>$OPAL_HOME/etc/opal.properties</filename>.
Opal will parse all the properties from opal.properties and make them
available to the job manager.</para>

<screen>
    /**
     * @param props the properties file containing the value to configure this plugin
     * @param config the opal configuration for this application
     * @param handle manager specific handle to bind to, if this is a resumption. 
     *               NULL,if this manager is being initialized for the first time.
     * 
     * @throws JobManagerException if there is an error during initialization
     */
    public void initialize(Properties props,
			   AppConfigType config,
			   String handle)
	throws JobManagerException;
</screen>
</listitem>

<listitem><para>The <filename>destroyJobManager</filename> is for cleanup
when the job manager is destroyed - all resources held should be freed
here.</para>

<screen>
    /**
     * @throws JobManagerException if there is an error during destruction
     */
    public void destroyJobManager()
	throws JobManagerException;
</screen>
</listitem>

<listitem><para>The <filename>launchJob</filename> method is for launching
a job with the given arguments. The input files are already staged in by
the service implementation, and the job manager implementation can assume
that they are already in the right location.</para>

<screen>
    /**
     * Launch a job with the given arguments. 
     *
     * @param argList a string containing the command line used to launch the application
     * @param numproc the number of processors requested. Null, if it is a serial job
     * @param workingDir String representing the working dir of this job on the local system
     * 
     * @return a plugin specific job handle to be persisted by the service implementation
     * @throws JobManagerException if there is an error during job launch
     */
    public String launchJob(String argList, 
			    Integer numproc, 
			    String workingDir)
	throws JobManagerException;
</screen>
</listitem>

<listitem><para>The <filename>waitForActivation</filename> method should
block until the job has begun execution. Opal uses this information to
collect job statistics.</para>

<screen>
    /**
     * @return status for this job after blocking
     * @throws JobManagerException if there is an error while waiting for the job to be ACTIVE
     */
    public StatusOutputType waitForActivation() 
	throws JobManagerException;
</screen>
</listitem>

<listitem><para>The <filename>waitForCompletion</filename> method should
block until the application has finished execution.</para>

<screen>
    /**
     * @return final job status
     * @throws JobManagerException if there is an error while waiting for the job to finish
     */
    public StatusOutputType waitForCompletion() 
	throws JobManagerException;
</screen>
</listitem>

<listitem><para>The <filename>destroyJob</filename> method should kill an
executing job.</para>

<screen>
    /**
     * @return final job status
     * @throws JobManagerException if there is an error during job destruction
     */
    public StatusOutputType destroyJob()
	throws JobManagerException;
</screen>
</listitem>
</orderedlist>

<para>More information about the Java classes can be obtained from the Javadocs, which you can generate from $OPAL_HOME as follows:</para>

<screen>
    ant api-docs
</screen>

<para>This will generate documentation of the Opal API, which will be
available in HTML form at
<filename>$OPAL_HOME/docs/api/index.html</filename>.</para>

<para>We recommend that you implement your particular job manager inside
the directory
<filename>$OPAL_HOME/src/edu/sdsc/nbcr/opal/manager</filename>. There you
will find other job managers that are available by default - you might want
to look at the <filename>ForkJobManager</filename> as an example. You can
compile your job manager with the rest of Opal, and install it within
Tomcat by running the following commands:</para>

<screen>
    ant compile
    ant install
</screen>
</section>
</chapter>

